{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding up a big array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a numpy array of $10^8$ random numbers, and add them up. How long does it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = int(1e8)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "arr = np.random.random(n_values)\n",
    "result = np.sum(arr)\n",
    "print(\"Result: \", result)\n",
    "\n",
    "duration = time.time() - start\n",
    "print(duration * 1000, \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given how fast was, how many numbers could we add up in an hour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_values / duration is number of values we sum up per second\n",
    "# multiply by 60 sec/min; 60 min/hour for number per hour\n",
    "print(\"{:g}\".format(n_values / duration * 60*60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "* Could we just put in that number for `n_values`, run it, and have the sum completed in an hour or so?\n",
    "* How much memory would be needed for that array?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using dask\n",
    "\n",
    "If you worked out how much memory would be required for that, you should have ended up with a number of the order of terabytes. My laptop definitely does not have terabytes of RAM. By splitting the problem up into smaller tasks, dask can can solve the whole problem with much less memory usage.\n",
    "\n",
    "Compare the code below to the code above -- nearly identical! Dask array mimic the numpy array API (as much as possible), making it easy to convery numpy code to dask code. Under the hood, dask uses numpy within each task, because numpy is already very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d_arr = da.random.random(n_values, chunks=n_values/10)\n",
    "dask_sum = da.sum(d_arr)\n",
    "print(\"Result:\", dask_sum.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_sum.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "* Why does the dask graph group these tasks into groups of 4 (on my laptop)?\n",
    "* What is the speedup for this process? How does that compare to the theoretical speedup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
